\documentclass[10pt,twoside]{article}

\usepackage[margin=1in]{geometry}
\usepackage{epsfig}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\title{6.867 Machine Learning  \subtitle{Homework 2} }

\maketitle

% **************************************************************************************************
 % Problem 1
% **************************************************************************************************

\section{\uppercase{Logistic Regression}}

\noindent Logistic Regression is a discriminative model used for classification. Given an input $x$, it finds the posterior of $x$ belonging to one of the classes and then uses that probability to classify $x$. In the simplest case, it takes a linear combination of the input $x$ and uses that as an input to the sigmoid function, which outputs a number between 0 and 1. One of the problems with logistic regressions is that they are very prone to overfitting to the training data. One way to prevent the overfitting is to add a regularization term, $\lambda$, which penalizes the size of the weight vector. The size of the weight vector can be penalized using the $L_1$ norm and the $L_2$ norm. Here, we explore how different values of $\lambda$  and the different norms affect several aspects of the logistic regression.

\begin{equation}

\sum_i log(1+exp(-y^{(i)}(wx^{(i)}+w_0))) + \lambda \left \| w \right \|_2^2

\end{equation}

\begin{equation}

\sum_{i=1}^n (\sigma (W^Tx^{(i)})-y^{(i)})x^{(i)} + 2\lambda 

\end{equation}


\begin{equation}

\frac{1}{1+e^{-x}}

\end{equation}


\subsection{Optimizing with Gradient Descent}

\noindent To investigate how $L_2$ regularization affected the logistic regression we tried $\lambda = 0$ and $\lambda = 1$. We decided not to penalize the bias term in the weight vector. We found that with $\lambda = 1$ the weight vector decreased in every iteration of the algorithm until it converged to its optimal value. We believe this is because for most iterations, the quickest way to decrease the objective function is to decrease the norm of the weight vector, because it is penalized by a quadratic factor. With $\lambda = 0$, the opposite happened. The norm of the weight increased in every iteration until it converged to its optimal value. Unregularized logistic regressions attempt to make the weight vector as large as possible because that makes the sigmoid function steeper, which in turn increases the log likelihood of the data. Our obersvations agree with our intuition that regularization makes the weight vector smaller.

\begin{figure}
       
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/1_1_L0.png}
                \caption{Graph of Norm of weight vector with lmbda = 0}
                \label{fig:tiger}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/1_1_L1.png}
                \caption{Graph of Norm of weight vector with lmbda = 0}
                \label{fig:mouse}
        \end{subfigure}
        \caption{Pictures of animals}\label{fig:animals}
\end{figure}

\subsection{Section1}


You should be able to concisely represent trends for those configurations. For example, you could have one plot showing the effect of the error rate for different regularizers and lambdas, a good example of changing the decision boundary, and general observations, and maybe a plot about the weights. Perform your experiments and see what makes the most sense.

\subsection{L1 vs L2 Norm}

Two common metrics used to penalize the size of the weight vector are the L1 and L2 norm. The norms are defined as follows: (L1 is sum of absolute values). We tested the effect of each of these norms and different lambda values on the Classification Error Rate, decision boundary, and the weights of the weight vector. We used scikit-learn's implementation of Logistic Regression, which also penalizes the bias term. To allow for unpenalized bias, the logistic regression has an additional parameter, intercept_scaling, which is a factor by which the intercept is multiplied by to offset the effects of the regularization. We set the value of the intercept scaling to be equal to the lambda value. We tested both the L1 and L2 model with lambda values ranging from 10^-5 to 10^5. 

Effect on Classification Error Rate:
In general, we found that the regularization seemed to have a larger effect on the CER when using the L1 norm then when using the L2 norm. The L2 norm logistic regression was able to maintain low CER's even with high lambda values. The L1 norm, on the other hand, often drove the weight vector to zero, which increased the CER. For most data sets, the L2 norm achieved a better CER than the L1 norm when set to the same lambda values. When both weight vectors were nonzero, their CER's usually only differed by a percentage point. The difference between the CER's grew dramatically whenever the L1 norm drove one of the weights in the weight vector to zero. The two models generally behaved that way except for the last data set, in which the data was linearly inseparable. There, the L1 and L2 norm achieved very similar CER scores for every lambda value we tested. 

\begin{figure}
       
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/B_CERL1.png}
                \caption{Classification Error Rate of L1 norm as a function of Lambda}
                \label{fig:tiger}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/B_CERL2.png}
                \caption{Classification Error Rate of L2 norm as a function of Lambda}
                \label{fig:mouse}
        \end{subfigure}
        \caption{Pictures of animals}\label{fig:animals}
\end{figure}

Effect on Decision Boundary 
As mentioned in the previous section, the L1 norm tends to drive the weights of the weight vector to zero as lambda increases. As a result, there were several instances where the decision boundary was simply the x axis CHECK THIS. In general, we found that decreasing lambda changed the decision boundary in a direction where it could better separate the training data. This was true for all of the data sets where the data was linearly separable. For the data set in which the data was linearly inseparable, the decision boundary never changed, regardless of the lambda value. 

\begin{figure}
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/LR1__1.png}
                \caption{A gull}
                \label{fig:gull}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/LR1_1.png}
                \caption{A gull2}
                \label{fig:gull2}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/LR1_10.png}
                \caption{A tiger}
                \label{fig:tiger}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P1/LR1_1000.png}
                \caption{A mouse}
                \label{fig:mouse}
        \end{subfigure}
        \caption{Pictures of animals}\label{fig:animals}
\end{figure}


Effect on Weights
Higher lambda values decreased the weights of the weight vector in both models. Higher lambda values affected the L1 model a lot more, as it was more likely to have zero valued weights. The L2 model was more likely to have very small, but still nonzero weights. 

\subsection{Optimal Parameters}
We evaluated the performance of both norms on different data sets using the same lambda values as before. We found that when using L1 regularization, a lambda value of 0.1 achieved the best performance on both the validation and the test sets. With L2 regularization the optimal value for lambda was 1. Both of these values can be modified by a factor of 10 without significantly hurting the accuracy. With these values, the two models achieved nearly identical results on all data sets, the difference in performance was never more than half a percentage point.

\subsection{Section2}



% **************************************************************************************************
 % Problem 2
% **************************************************************************************************

\section{\uppercase{Support Vector Machines}}

Support Vector Machines are supervised learning models that work by finding a dividing hyperplane between the training data while maximizing the gap between the training data and the decision boundary. This is to help the classifier generalize better and makes it more robust to noise. Assuming the data is linearly separable, finding this dividing hyperplane amounts to solving the quadratic program

% \begin{equation}
% \min_{w,b} \frac{1}{2} ||w||^2 s.t. y^i(w^T x^i + b) \gte 1, 1 \lte i \lte n
% \end{equation}

% \begin{equation}
% \begin{array}
% min \frac{1}{2} ||w||^2 &\\
% s.t. y^i(w^T x^i + b) \geq 1, 1 \leq i \leq n
% \end{array}
% \end{equation}

\begin{equation}
\begin{array}{ll@{}ll}
\text{min}  & \displaystyle \frac{1}{2} ||w||^2 &\\
\text{s. t.}& \displaystyle y^i(w^T x^i + b) \geq 1 , 1 \leq i \leq n
\end{array}
\end{equation}

where $w$ is the vector perpendicular to the dividing hyperplane and $\frac{1}{||w||}$ is the size of the margin.

If the data is almost but not completely linearly separable, we can still model the data with an SVM by introducing slack variables when solving for a classifier. We allow the training points to be misclassified by some amount $e$ and the goal is to maximize the margin while minimizing the slack. This formulation is called C-SVM and the separating hyperplane can be found by solving the quadratic program below.

\begin{equation}
\begin{array}{ll@{}ll}
\text{min}  & \displaystyle \frac{1}{2} ||w||^2 + C \sum_i e_i &\\
\text{s. t.}& \displaystyle y^i(w^T x^i + b) \geq 1 - e_i , 1 \leq i \leq n \\
& e_i \geq 0 , 1 \leq i \leq n
\end{array}
\end{equation}

\subsection{Dual Formulation of C-SVM}
This problem as stated above is difficult to implement with kernels functions. Fortunately, we convert the problem from the primal formulation stated above and instead implement its dual formuation.

\begin{equation}
\begin{array}{ll@{}ll}
\text{min}  & \displaystyle \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^n \alpha_i \alpha_j y_i y_j [\phi(x_i)^T \phi(x_j)] - \sum_{t=1}^{n} \alpha_t &\\
\text{s. t.}& \displaystyle 0 \leq a_t \leq C , \sum_{t=1}^{n} \alpha_t y_t = 0
\end{array}
\end{equation}

To solve this linear program and get the $\alpha$s, we used the solver in a python library called cvxopt. The solver finds solutions to quadratic programs of the form:

\begin{equation}
\begin{array}{ll@{}ll}
\text{min}  & \displaystyle \frac{1}{2} x^T P x + q^T x &\\
\text{s. t.}& \displaystyle G x \leq h &\\
& Ax = b
\end{array}
\end{equation}

In our case, our matrices are 

\begin{equation*}
P = Diag(\overrightarrow{y}) X X^T Diag(\overrightarrow{y})
\end{equation*}
\begin{equation*}
q = -1 * \overrightarrow{1}
\end{equation*}
\begin{equation*}
G = [I | -I]
\end{equation*}
\begin{equation*}
h = [C * \overrightarrow{1} | \overrightarrow{0}]
\end{equation*}
\begin{equation*}
A = \overrightarrow{y}
\end{equation*}
\begin{equation*}
b = 0
\end{equation*}

$Diag(\overrightarrow{y})$ is a diagonal matrix with $y^{(i)}$s on the diagonal.

This solver returns $\overrightarrow{x}$ which is our $\overrightarrow{\alpha}$ vector. Testing this implementation,

Do the stupid example!

\subsection{2D Dataset Results}

\begin{figure}
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data1_test_C1.png}
                \caption{A gull}
                \label{fig:gull}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data2_test_C1.png}
                \caption{A gull2}
                \label{fig:gull2}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data3_test_C1.png}
                \caption{A tiger}
                \label{fig:tiger}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data4_test_C1.png}
                \caption{A mouse}
                \label{fig:mouse}
        \end{subfigure}
        \caption{Pictures of animals}\label{fig:animals}
\end{figure}


\subsection{2D Dataset Results with Kernel Functions}

small lambdas penalize the w less, which let w get bigger

as Lambda --- > inf
   w -------> 0
   margin = 1/w -------> inf

   Lambda -----> 0
   Empirical evidence: as lambda increases, the margin decreases

% **************************************************************************************************
 % Problem 3
% **************************************************************************************************

\section{\uppercase{Support Vector Machine with Pegasos}}

Even though SVMs have some nice properties, it can be very inefficient to solve large SVMs. The Pegasos Algorithm can be used to solve a formulation of Soft-SVM that is equivalent to C-SVM with C = 1/(lmbda*n). One of the biggest advantages the Pegasos algorithm offers is very fast training times. 

\subsection{Linear SVM}
We implemented the Pegasos algorithm to solve the objective function shown above and tested several values of lambda between 2^1 and 2^-10 and found that the margin shrank as lambda was increased. 


\subsection{Kernelized Pegasos}
The Pegasos algorithm can be easily modified to support Kernels. We implemented this modification and evaluated the performance of SVMs trained using Pegasos. 


% **************************************************************************************************
 % Problem 4
% **************************************************************************************************

\section{\uppercase{Handwritten Digit Recognition with MNIST}}


\vfill
\end{document}

