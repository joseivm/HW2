\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}\uppercase {Logistic Regression}}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Optimizing with Gradient Descent}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pictures of animals\relax }}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Section1??}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}L1 vs L2 Norm}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pictures of animals\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gull}{{3a}{3}}
\newlabel{sub@fig:gull}{{a}{3}}
\newlabel{fig:gull2}{{3b}{3}}
\newlabel{sub@fig:gull2}{{b}{3}}
\newlabel{fig:tiger}{{3c}{3}}
\newlabel{sub@fig:tiger}{{c}{3}}
\newlabel{fig:mouse}{{3d}{3}}
\newlabel{sub@fig:mouse}{{d}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Pictures of animals\relax }}{3}}
\newlabel{fig:animals}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Optimal Parameters}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Section2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\uppercase {Support Vector Machines}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dual Formulation of C-SVM}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces SVM with linear kernel on four data points. \relax }}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}2D Dataset Results}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Linear SVM perfomance of different datasets. Datasets 1 and 3 are relatively linearly separable so this SVM performs well on them. Dataset 2 and 4 are definitely not linearly separable so this SVM is not the right model for the data.\relax }}{5}}
\newlabel{fig:animals}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}2D Dataset Results with Kernel Functions}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Effect of C on Margin}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Decision boundary for SVM with linear kernel. The margin clearly gets smaller for large C\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Decision boundary for SVM with Gaussian RBF kernel. In figure a, the margins are so big they almost do not show up in the graph. By figure c, they have shrunk considerably closer to the decision boundary.\relax }}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\uppercase {Support Vector Machine with Pegasos}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear SVM}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Kernelized Pegasos}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\uppercase {Handwritten Digit Recognition with MNIST}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Logistic Regression vs Linear SVM}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Effect of C on Margin}{6}}
