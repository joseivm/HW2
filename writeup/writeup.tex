\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}
\usepackage[small]{caption}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{6.867 Machine Learning  \subtitle{Homework 2} }

\maketitle

% **************************************************************************************************
 % Problem 1
% **************************************************************************************************

\section{\uppercase{Logistic Regression}}

\noindent Logistic Regression is a discriminative model used for classification. Given an input x, it finds the posterior of x belonging to one of the classes and then uses that probability to classify x. In the simplest case, it takes the dot product of x and w and uses that as an input to the sigmoid function, which outputs a number between 0 and 1. An advantage of logistic regressions is that they have few parameters, which allows them to be trained relatively quickly. One of the problems with logistic regressions is that they are very prone to overfitting to the training data. One way to prevent the overfitting is to add a regularization term, lambda, which penalizes the size of the weight vector. The size of the weight vector can be penalized using the L1 norm of the L2 norm. Here, we explore how different lambda values and the different norms affect several aspects of the logistic regression.

\subsection{Optimizing with Gradient Descent}

\noindent To investigate how regularization affected the logistic regression we tried lambda values of 0 and 1. We decided not to penalize the bias term in the weight vector. As expected, we found that with a lambda value of 1 the weight vector decreased in every iteration of the algorithm. 


\subsection{Section1}

\subsection{Section2}


% **************************************************************************************************
 % Problem 2
% **************************************************************************************************

\section{\uppercase{Support Vector Machines}}

Support Vector Machines are supervised learning models that work by finding a dividing hyperplane between the training data while maximizing the gap between the training data and the decision boundary. This is to help the classifier generalize better and makes it more robust to noise. Assuming the data is linearly separable, finding this dividing hyperplane amounts to solving the quadratic program

% \begin{equation}
% \min_{w,b} \frac{1}{2} ||w||^2 s.t. y^i(w^T x^i + b) \gte 1, 1 \lte i \lte n
% \end{equation}

% \begin{equation}
% \begin{array}
% min \frac{1}{2} ||w||^2 &\\
% s.t. y^i(w^T x^i + b) \geq 1, 1 \leq i \leq n
% \end{array}
% \end{equation}

\begin{equation}
\begin{array}{ll@{}ll}
\text{min}  & \displaystyle \frac{1}{2} ||w||^2 &\\
\text{s. t.}& \displaystyle y^i(w^T x^i + b) \geq 1 , 1 \leq i \leq n
\end{array}
\end{equation}

where $w$ is the vector perpendicular to the dividing hyperplane and $\frac{1}{||w||}$ is the size of the margin.

If the data is almost but not completely linearly separable, we can still model the data with an SVM by introducing slack variables when solving for a classifier. We allow the training points to be misclassified by some amount $e$ and the goal is to maximize the margin while minimizing the slack. This formulation is called C-SVM and the separating hyperplane can be found by solving the quadratic program 

% \begin{equation}
% \min_{w,b} \frac{1}{2} ||w||^2 s.t. y^i(w^T x^i + b) \gte 1, 1 \lte i \lte n
% \end{equation}

 The introduction of a slack variable, (insert slack), allows the misclassify some training samples. With the slack variables, the objective function becomes:

In the supervised learning problem of regression, we are given a set of $n$ data points and $n$ target values and the goal is to find a function that relates $x$ to $y$. We have to do this in such a way that this function generalizes well to unseen values of $x$. Linear Basis Function Regression aims to find the optimal linear combination of basis functions to create a function mapping $x$ to $y$. This linear combination is expressed as a vector of weights for each of these basis functions. These basis functions take the form

\subsection{Blah}


\subsection{Blah2}


\subsection{Section?}



% **************************************************************************************************
 % Problem 3
% **************************************************************************************************

\section{\uppercase{Support Vector Machine with Pegasos}}

\subsection{Section1?}


\subsection{Section2?}


% **************************************************************************************************
 % Problem 4
% **************************************************************************************************

\section{\uppercase{Handwritten Digit Recognition with MNIST}}


\vfill
\end{document}

